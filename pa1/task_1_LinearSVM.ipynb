{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CENG501 - Spring2021 - PA1 - Task 1 - LinearSVM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/berkeracir/ceng501/blob/main/pa1/task_1_LinearSVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1X4X2hmaEmip"
      },
      "source": [
        "# Task 1: Implement Linear Classification with Hinge Loss \n",
        "# CENG501 - Spring 2021 - PA1\n",
        "\n",
        "In this task, you will implement a linear classification model with max-margin loss and regularization penalty, a.k.a. SVM Loss. \n",
        "\n",
        "*Disclaimer: Many components in this notebook are adapted or taken from [CS231n](https://cs231n.github.io/) materials.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRaKrjO5JSu-"
      },
      "source": [
        "## 1 Import the Modules\n",
        "\n",
        "Let us start with importing some libraries that we will use throughout the task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4L5nogMKyNx"
      },
      "source": [
        "import matplotlib.pyplot as plt # For plotting\n",
        "import numpy as np              # NumPy, for working with arrays/tensors \n",
        "import os                       # Built-in library for filesystem access etc.\n",
        "import pickle                   # For (re)storing Python objects into (from) files \n",
        "import time                     # For measuring time\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = [12, 8]\n",
        "plt.rcParams['figure.dpi'] = 100 # 200 e.g. is really fine, but slower"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMmi17e-JX7o"
      },
      "source": [
        "## 2 The Dataset\n",
        "\n",
        "In this task, we will use the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset. CIFAR-10 is a relatively small dataset frequently used for proof-of-concept experiments. It consists of 10 object classes ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck') with equal number of samples (i.e. a balanced dataset). There are 60,000 32x32 colour images, with 6,000 images per class. \n",
        "\n",
        "The dataset provides a split of 50,000 training images and 10,000 test images. For each class, there are 5,000 images for training and 1,000 images for testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0mOMB0zOsTp"
      },
      "source": [
        "### 2.1 Download the Dataset\n",
        "\n",
        "Thankfully, we can directly download the dataset as follows. However, sometimes access to the dataset somehow doesn't work. It is best if you save a downloaded version of the dataset and use it if you have access problems. Using the following cell is faster, however, as the file is directly downloaded to a Google server.\n",
        "\n",
        "After downloading and uncompressing the file, check the file and the folders using the menu on the left."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBMu6-6mIifW"
      },
      "source": [
        "# Download the file\n",
        "! wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
        "\n",
        "# Extract the compressed file\n",
        "! tar zxvf cifar-10-python.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJi0EAJtOvj4"
      },
      "source": [
        "### 2.2 Load the Dataset\n",
        "\n",
        "Let us load the images from the filesystem. Note that there are many neat loaders like [torchvision.datasets](https://pytorch.org/vision/stable/datasets.html). For the time being, we will stick to doing things the hard way.\n",
        "\n",
        "Pay attention to the sizes of the tensors after they are loaded. An image in the CIFAR10 dataset has size 32x32x3 and we will reshape an image into a 1D vector with 3072 elements.\n",
        "\n",
        "*Disclaimer: This loader is adapted from [CS231n](https://cs231n.github.io/) materials.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75JGmmE8G90g"
      },
      "source": [
        "def load_CIFAR_batch(filename):\n",
        "  \"\"\" load single batch of cifar \"\"\"\n",
        "  with open(filename, 'rb') as f:\n",
        "    datadict = pickle.load(f, encoding='latin1')\n",
        "    X = datadict['data']\n",
        "    Y = datadict['labels']\n",
        "    X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
        "    Y = np.array(Y)\n",
        "    return X, Y\n",
        "\n",
        "def load_CIFAR10(ROOT):\n",
        "  \"\"\" load all of cifar \"\"\"\n",
        "  xs = []\n",
        "  ys = []\n",
        "  for b in range(1,6):\n",
        "    f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
        "    X, Y = load_CIFAR_batch(f)\n",
        "    xs.append(X)\n",
        "    ys.append(Y)    \n",
        "  Xtr = np.concatenate(xs)\n",
        "  Ytr = np.concatenate(ys)\n",
        "  del X, Y\n",
        "  Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
        "  return Xtr, Ytr, Xte, Yte\n",
        "\n",
        "def get_CIFAR10_data(cifar10_dir, num_training=49000, num_validation=1000, num_test=1000):\n",
        "    \"\"\"\n",
        "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
        "    it for classifiers. These are the same steps as we used for the SVM, but\n",
        "    condensed to a single function.\n",
        "    \"\"\"\n",
        "    # Load the raw CIFAR-10 data\n",
        "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
        "        \n",
        "    # Subsample the data\n",
        "    mask = range(num_training, num_training + num_validation)\n",
        "    X_val = X_train[mask]\n",
        "    y_val = y_train[mask]\n",
        "    mask = range(num_training)\n",
        "    X_train = X_train[mask]\n",
        "    y_train = y_train[mask]\n",
        "    mask = range(num_test)\n",
        "    X_test = X_test[mask]\n",
        "    y_test = y_test[mask]\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
        "\n",
        "# Now use these functions to load the dataset:\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data('cifar-10-batches-py/')\n",
        "\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
        "X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
        "\n",
        "# Note that 32x32x3 = 3072\n",
        "print('Training data shape: ', X_train.shape)\n",
        "print('Training labels shape: ', y_train.shape)\n",
        "print('Test data shape: ', X_test.shape)\n",
        "print('Test labels shape: ', y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2olSQDZjOycC"
      },
      "source": [
        "### 2.3 Visualize Some Samples\n",
        "\n",
        "For sanity check, let us visualize some samples.\n",
        "\n",
        "*Disclaimer: This code piece is taken from [CS231n](https://cs231n.github.io/) materials.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_-RGheIFrCf"
      },
      "source": [
        "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "num_classes = len(classes)\n",
        "samples_per_class = 7\n",
        "for y, cls in enumerate(classes):\n",
        "    idxs = np.flatnonzero(y_train == y)\n",
        "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
        "    for i, idx in enumerate(idxs):\n",
        "        plt_idx = i * num_classes + y + 1\n",
        "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
        "        plt.imshow(X_train[idx].reshape(32,32,3).astype('uint8'))\n",
        "        plt.axis('off')\n",
        "        if i == 0:\n",
        "            plt.title(cls)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZSPB1jyVZ-H"
      },
      "source": [
        "### 2.4 Preprocessing\n",
        "\n",
        "We will perform two preprocessing steps: Subtracting the mean and adding a bias dimension to the inputs to make things easier for calculations on the weight matrix.\n",
        "\n",
        "Warning/note: Applying mean substraction again will produce a black image. Why?\n",
        "\n",
        "*Disclaimer: This step is taken from [CS231n](https://cs231n.github.io/) materials.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmSzBuJeVrVh"
      },
      "source": [
        "# Preprocessing: subtract the mean image\n",
        "# first: compute the image mean based on the training data\n",
        "mean_image = np.mean(X_train, axis=0)\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.imshow(mean_image.reshape((32,32,3)).astype('uint8')) # visualize the mean image\n",
        "plt.show()\n",
        "\n",
        "# second: subtract the mean image from train and test data\n",
        "X_train -= mean_image\n",
        "X_test -= mean_image\n",
        "X_val -= mean_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzGt3qw0WBc0"
      },
      "source": [
        "# third: append the bias dimension of ones (i.e. bias trick) so that our SVM\n",
        "# only has to worry about optimizing a single weight matrix W.\n",
        "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
        "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
        "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
        "\n",
        "print('Training data shape: ', X_train.shape)\n",
        "print('Validation data shape: ', X_val.shape)\n",
        "print('Test data shape: ', X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fycAQyVdJaHR"
      },
      "source": [
        "## 3 Construct the Model\n",
        "\n",
        "In this task, we are defining a linear model for classification, as we have seen in the lectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qm3vEgJ7JgZZ"
      },
      "source": [
        "### 3.1 A Linear Model\n",
        "\n",
        "A linear model performs a simple transformation on input $\\mathbf{x}\\in R^D$ using parameters $W \\in R^{C\\times D}$, where $D$ is the dimensionality of the input and $C$ is the number of classes. This transformation can be formally denoted as (note that $\\mathbf{x}$ contains a constant 1 for the bias):\n",
        "$$\n",
        "f(\\mathbf{x}; W) = W\\cdot \\mathbf{x}. \n",
        "$$\n",
        "\n",
        "If we look at this operation for class $c$, we see that this is simply a dot product:\n",
        "$$\n",
        "f(\\mathbf{x}; \\mathbf{w}_c)_c = \\mathbf{w}_c \\cdot \\mathbf{x} = \\sum_i {w}_{ci} x_i. \n",
        "$$\n",
        "\n",
        "Let us first implement this linear operation as a linear layer. In the following cell, complete the \"@TODO\" parts of `forward_naive()` and `forward_vectorized()` functions. You can ignore the `update()` and `predict()` functions for the time being.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ebrij4IDOmLw"
      },
      "source": [
        "class LinearLayer:\n",
        "\n",
        "  def __init__(self, D, C):\n",
        "    \"\"\"\n",
        "      Initialize the linear layer. Weights are randomly initialized in a small range.\n",
        "\n",
        "      D: The dimensionality of a single instance. \n",
        "      C: The number of classes.\n",
        "    \"\"\"\n",
        "    self.D = D\n",
        "    self.C = C\n",
        "    np.random.seed(501) # for reproducibility, set the seed to a constant\n",
        "    self.W = 0.001 * np.random.randn(D, C)\n",
        "\n",
        "  def forward_naive(self, X, W=None):\n",
        "    \"\"\"\n",
        "    Use the current weights to obtain the scores for a batch of samples (X). \n",
        "    Use iterations in this version. Iterate over samples & classes & dimensions of X.\n",
        "\n",
        "    Inputs:\n",
        "    - X: A numpy array of shape (N, D) containing N samples each of dimension D.\n",
        "    Returns:\n",
        "    - scores: A numpy array of shape (N, C). Predicted scores for the data in X.\n",
        "    \"\"\"\n",
        "    scores = np.zeros((X.shape[0], self.C))\n",
        "    N = X.shape[0] # number of samples\n",
        "    if W is None: W = self.W\n",
        "\n",
        "    ###########################################################################\n",
        "    # @TODO: Implement this method. Store the scores in 'scores' array.       #\n",
        "    # Hint: You will have three loops nested here. One loop over samples, one #\n",
        "    # over classes and one over dimensions.                                   #\n",
        "    ###########################################################################\n",
        "    pass\n",
        "    ###########################################################################\n",
        "    #                           END OF YOUR CODE                              #\n",
        "    ###########################################################################\n",
        "    return scores\n",
        "\n",
        "  def forward_vectorized(self, X, W=None):\n",
        "    \"\"\"\n",
        "    Use the current weights to obtain the scores for a batch of samples (X).\n",
        "\n",
        "    Inputs:\n",
        "    - X: A numpy array of shape (N, D) containing N samples each of dimension D.\n",
        "    Returns:\n",
        "    - scores: A numpy array of shape (N, C). Predicted scores for the data in X. \n",
        "    \"\"\"\n",
        "    scores = np.zeros((X.shape[0], self.C))\n",
        "    if W is None: W = self.W\n",
        "\n",
        "    ###########################################################################\n",
        "    # @TODO: Implement this method. Store the scores in 'scores' array.       #\n",
        "    # Hint: This should be a single line of code using numpy functions.       #\n",
        "    ###########################################################################\n",
        "    pass\n",
        "    ###########################################################################\n",
        "    #                           END OF YOUR CODE                              #\n",
        "    ###########################################################################\n",
        "    return scores\n",
        "\n",
        "  def predict(self, X):\n",
        "    \"\"\"\n",
        "    Obtain the predictions as discrete labels (integers)\n",
        "\n",
        "    Inputs:\n",
        "    - X: A numpy array of shape (N, D) containing N samples each of dimension D.\n",
        "    Returns:\n",
        "    - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n",
        "      array of length N, and each element is an integer giving the predicted\n",
        "      class.\n",
        "    \"\"\"\n",
        "    y_pred = np.zeros(X.shape[0])\n",
        "\n",
        "    ###########################################################################\n",
        "    # @TODO: Implement this method. Store the predicted labels in y_pred.     #\n",
        "    # Hint: You can use forward() to estimate the scores and the predicted    # \n",
        "    # label is the class (integer) that has the highest score. Write this in  #\n",
        "    # vectorized form in a single line. Check function np.argmax()            #\n",
        "    ###########################################################################\n",
        "    pass\n",
        "    ###########################################################################\n",
        "    #                           END OF YOUR CODE                              #\n",
        "    ###########################################################################\n",
        "    return y_pred\n",
        "\n",
        "  def update(self, dW, learning_rate):\n",
        "    \"\"\"\n",
        "    Update the model parameters with the gradients (dW) by scaling with the \n",
        "    learning rate.\n",
        "\n",
        "    Inputs:\n",
        "    - dW: The derivative of the loss wrt. W. The shapes of self.W and dW must match.\n",
        "    - learning_rate: The learning rate.\n",
        "    \"\"\"\n",
        "    #########################################################################\n",
        "    # @TODO:                                                                 #\n",
        "    # Update the weights using the gradient and the learning rate.          #\n",
        "    #########################################################################\n",
        "    pass\n",
        "    #########################################################################\n",
        "    #                       END OF YOUR CODE                                #\n",
        "    #########################################################################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l17hBE-la0Rv"
      },
      "source": [
        "Let us compare the `forward_naive()` and `forward_vectorized()` functions. We will compare the two implementations in terms of time and outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gk_TtY-Sa4JU"
      },
      "source": [
        "D = X_train.shape[1] # dimensionality\n",
        "C = np.max(y_test) + 1 # labels are in 0-9, so we add 1\n",
        "N = X_train.shape[0]\n",
        "\n",
        "# Over a set of 10 samples\n",
        "sample_indices = np.random.choice(range(N), size=10)\n",
        "X_samples = X_train[sample_indices]\n",
        "\n",
        "model = LinearLayer(D, C)\n",
        "\n",
        "# First the naive method\n",
        "tic = time.time()\n",
        "scores_naive = model.forward_naive(X_samples)\n",
        "toc = time.time()\n",
        "print(f\"forward_naive() took {toc-tic}s!\")\n",
        "\n",
        "# Now the vectorized method\n",
        "tic = time.time()\n",
        "scores = model.forward_vectorized(X_samples)\n",
        "toc = time.time()\n",
        "print(f\"forward_vectorized() took {toc-tic}s!\")\n",
        "\n",
        "# Now check whether they have produced the same results\n",
        "print(\"\\nYou should see zero difference between the scores calculated using the\\n\"\n",
        "      \"naive method and the vectorized method: %f\\n\" % \n",
        "      np.sum((scores_naive-scores)**2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jMASapzYYOl"
      },
      "source": [
        "You should obtain a difference of zero between the scores returned by the two implementations. You should observe that there is a 300x difference between the running times of the two implementations. For that reason, we should always prefer vectorized implementations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tSQEAw6JloK"
      },
      "source": [
        "### 3.2 The Loss Function and the Gradients\n",
        "\n",
        "In this task, we will train the linear model using Hinge Loss and L2 regularization on the weights, a.k.a. SVM Loss. Formally, we can define this loss as follows:\n",
        "$$\n",
        "L = \\frac{1}{N} \\sum_{i=1}^N \\sum_{j\\neq y_i} \\max(0, s_j - s_{y_i} + \\Delta) + \\frac{\\lambda}{2}\\sum_{i,j} w_{i,j}^2,\n",
        "$$\n",
        "where $s_j = \\sum_k w_{jk} x_k$ is the score for class $j$; $y_i$ is the correct class for sample $\\mathbf{x}_i$; $\\Delta$ is the margin for max-margin loss and $\\lambda$ is the regularization coefficient. With SVM Loss, we will take the margin as 1 (one), but we will leave it as a hyper-parameter in our implementations.\n",
        "\n",
        "Pay close attention to the loss definition in the above equation. Any discrepancy in your implementation will lead to different outputs. Note that the following implementation includes redundancies in that same quantities are calculated repeatedly. In frameworks, these redundancies are avoided by creating \"computation graphs\" and performing calculations over the computation graph. However, to simplify the implementation and to make the learning parts more explicit, I chose to trade efficiency.\n",
        "\n",
        "In the following code segment, implement the `calculate()` and `gradient()` functions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "694vNGbZYmfJ"
      },
      "source": [
        "class SVM_Loss:\n",
        "  def __init__(self, margin, reg):\n",
        "    self.margin = margin\n",
        "    self.reg = reg\n",
        "  \n",
        "  def calculate(self, W, scores, y):\n",
        "    \"\"\"\n",
        "    Hinge loss with L2 regularization. Follow the formulation above closely.\n",
        "\n",
        "    Inputs: \n",
        "    - scores: A numpy array with shape (N, C). The scores for each class for each sample.\n",
        "    - y: A numpy array of integers, shape (N). Stores correct labels for samples.\n",
        "\n",
        "    Output: - loss: A floating point number, representing the total loss for the samples.\n",
        "    \"\"\"\n",
        "    loss = 0.0\n",
        "    N = y.size\n",
        "    reg = self.reg\n",
        "    margin = self.margin\n",
        "\n",
        "    #############################################################################\n",
        "    # @TODO:                                                                    #\n",
        "    # Implement a vectorized version of the SVM loss in the above equation,     #\n",
        "    # storing the result in variable `loss`                                     #\n",
        "    #############################################################################\n",
        "    pass\n",
        "    #############################################################################\n",
        "    #                             END OF YOUR CODE                              #\n",
        "    #############################################################################\n",
        "    \n",
        "    return loss\n",
        "\n",
        "  def gradient(self, W, X, y, scores):\n",
        "    \"\"\"\n",
        "    Calculates the gradient of the loss wrt. the weights, W. \n",
        "\n",
        "    Inputs:\n",
        "    - X: A numpy array of shape (N, D). The input samples over which we are calculating the gradient.\n",
        "    - y: A numpy array of integers, shape (N). Stores correct labels for samples.\n",
        "    - scores: A numpy array with shape (N, C). The scores for each class for each sample.\n",
        "    \n",
        "    Output:\n",
        "    - dW: A numpy array with shape (D, C), same as the shape of W. Stores the gradient\n",
        "    of the loss wrt. W.\n",
        "    \"\"\"\n",
        "    reg = self.reg\n",
        "    margin = self.margin\n",
        "    dW = np.zeros(W.shape) # initialize the gradient as zero\n",
        "    N = y.size\n",
        "\n",
        "    #############################################################################\n",
        "    # TODO:                                                                     #\n",
        "    # Implement a vectorized version of the gradient for the structured SVM     #\n",
        "    # loss, storing the result in dW.                                           #\n",
        "    #############################################################################\n",
        "    pass \n",
        "    #############################################################################\n",
        "    #                             END OF YOUR CODE                              #\n",
        "    #############################################################################\n",
        "\n",
        "    return dW"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXwT-6d2xPX-"
      },
      "source": [
        "### 3.3 Check the Gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9fFCyLojIiZ"
      },
      "source": [
        "Let us check whether your implementation was correct. For this, we will calculate the gradient numerically and compare it with the analytical gradient we have obtained. \n",
        "\n",
        "Remember from your calculus courses that the derivative of a function $f()$ at $x$ can be calculated numerically as follows:\n",
        "\n",
        "$$\n",
        "\\frac{df(x)}{dx} = \\lim_{h\\rightarrow \\infty} \\frac{f(x+h)-f(x)}{h},\n",
        "$$\n",
        "\n",
        "which can be approximated by picking a sufficiently small $h$ value: \n",
        "\n",
        "$$\n",
        "\\frac{df(x)}{dx} \\simeq \\frac{f(x+h)-f(x)}{h}.\n",
        "$$\n",
        "\n",
        "In practice, a symmetric version is used for better estimatation of the derivative:\n",
        "$$\n",
        "\\frac{df(x)}{dx} \\simeq \\frac{f(x+h)-f(x-h)}{2h}.\n",
        "$$\n",
        "\n",
        "In the following, you should see differences between the analytical and numerical gradients less than $10^{-10}$ except for a few dimensions as $max()$ function is not strictly differentiable and numerical derivative can slightly differ from the analytic one near the discontinuity. \n",
        "\n",
        "If you see low errors, you can assume that our gradient implementation was correct."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqEVXzt7jI0u"
      },
      "source": [
        "from random import randrange\n",
        "\n",
        "def grad_check_sparse(f, x, analytic_grad, num_checks=10, h=1e-5):\n",
        "  \"\"\"\n",
        "  sample a few random elements and only return numerical\n",
        "  in these dimensions.\n",
        "  \"\"\"\n",
        "\n",
        "  for i in range(num_checks):\n",
        "    ix = tuple([randrange(m) for m in x.shape])\n",
        "    oldval = x[ix]\n",
        "\n",
        "    x[ix] = oldval + h # increment a single dimension of x (W) by h\n",
        "    fxph = f(x) # evaluate f(x + h)\n",
        "\n",
        "    x[ix] = oldval - h # decrement by h\n",
        "    fxmh = f(x) # evaluate f(x - h)\n",
        "    \n",
        "    x[ix] = oldval # reset\n",
        "\n",
        "    grad_numerical = (fxph - fxmh) / (2 * h)\n",
        "    grad_analytic = analytic_grad[ix]\n",
        "    rel_error = abs(grad_numerical - grad_analytic) / (abs(grad_numerical) + abs(grad_analytic))\n",
        "    print('numerical: %f analytic: %f, relative error: %e' % (grad_numerical, grad_analytic, rel_error))\n",
        "\n",
        "def f(W): \n",
        "  scores = model.forward_vectorized(X_test, W)\n",
        "  return loss_fn.calculate(W, scores, y_test) #W, scores, y):\n",
        "\n",
        "# Create \n",
        "model = LinearLayer(D, C)\n",
        "loss_fn = SVM_Loss(margin=1, reg=1e-5)\n",
        "scores = model.forward_vectorized(X_test)\n",
        "dW = loss_fn.gradient(model.W, X_test, y_test, scores)\n",
        "grad_numerical = grad_check_sparse(f, x=model.W, analytic_grad=dW, num_checks=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9pxbgMXJo8k"
      },
      "source": [
        "### 3.4 Training Method\n",
        "\n",
        "Okay, it seems that we have most of the ingredients ready, except for a few things. Go back to the implementation of the `LinearModel` and complete the `update()` function. Then come back here and define the train function (just click run)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Uz286SqU8Ae"
      },
      "source": [
        "def sample_batch(X, y, batch_size):\n",
        "  \"\"\"Get a random batch of size batch_size from (X, y).\"\"\"\n",
        "  batch_indices = np.random.choice(range(X.shape[0]), size=batch_size)\n",
        "  X_batch = X[batch_indices]\n",
        "  y_batch = y[batch_indices]\n",
        "\n",
        "  return X_batch, y_batch\n",
        "\n",
        "def train(model, loss_fn, X, y, learning_rate=1e-3, epochs=10, batch_size=32, verbose=False):\n",
        "  \"\"\"\n",
        "    Train this linear classifier using stochastic gradient descent.\n",
        "    Inputs:\n",
        "    - X: A numpy array of shape (N, D) containing training data; there are N\n",
        "      training samples each of dimension D.\n",
        "    - y: A numpy array of shape (N,) containing training labels; y[i] = c\n",
        "      means that X[i] has label 0 <= c < C for C classes.\n",
        "    - learning_rate: (float) learning rate for optimization.\n",
        "    - reg: (float) regularization strength.\n",
        "    - num_iters: (integer) number of steps to take when optimizing\n",
        "    - batch_size: (integer) number of training examples to use at each step.\n",
        "    - verbose: (boolean) If true, print progress during optimization.\n",
        "    Outputs:\n",
        "    A list containing the value of the loss function at each training iteration.\n",
        "  \"\"\"\n",
        "  num_train, dim = X.shape\n",
        "\n",
        "  # Run stochastic gradient descent to optimize W\n",
        "  loss_history = []\n",
        "  for epoch in range(epochs):\n",
        "    for it in range(int(num_train/batch_size)): \n",
        "        \n",
        "      # Get a batch of samples\n",
        "      X_batch, y_batch = sample_batch(X, y, batch_size)\n",
        "\n",
        "      # Feed-forward through the model\n",
        "      scores = model.forward_vectorized(X_batch)\n",
        "\n",
        "      # Calculate the loss\n",
        "      loss = loss_fn.calculate(model.W, scores, y_batch)\n",
        "      loss_history.append(loss)\n",
        "\n",
        "      # Calculate the gradient\n",
        "      dW = loss_fn.gradient(model.W, X_batch, y_batch, scores)\n",
        "\n",
        "      # perform parameter update\n",
        "      model.update(dW, learning_rate)\n",
        "\n",
        "    if verbose: print(f'Epoch {epoch} / {epochs}: {loss}')\n",
        "      \n",
        "  return loss_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUmdAA1QJvRx"
      },
      "source": [
        "## 4 Train the Model\n",
        "\n",
        "Now, all the pieces are ready and we can train the model. This will be very easy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Gg_Xp3vrg42"
      },
      "source": [
        "num_of_classes = np.max(y_test) + 1 # assume y takes values 0...K-1 where K is number of classes\n",
        "num_of_samples, num_of_dim = X_train.shape\n",
        "\n",
        "model = LinearLayer(num_of_dim, num_of_classes)\n",
        "loss_fn = SVM_Loss(margin=1, reg=1e3)\n",
        "\n",
        "loss_history = train(model, loss_fn, X_train, y_train, learning_rate=1e-5, epochs=7, batch_size=200, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svoFJa6yJysC"
      },
      "source": [
        "## 5 Analyze Results\n",
        "\n",
        "We have trained our model and now we should analyze how well it performed. We should look at several factors:\n",
        "\n",
        "* The loss curve.\n",
        "* Quantitative analysis of the performance.\n",
        "* Visual analysis of the weights and the predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DILBn6y92Ceg"
      },
      "source": [
        "### 5.1 The Loss Curve\n",
        "\n",
        "One of the first things we should do when analyzing a model is to plot the loss curve. We should ideally see a smoothly decreasing curve over iterations/epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNZBSrzXiVVe"
      },
      "source": [
        "plt.plot(loss_history)\n",
        "plt.xlabel('Iteration number')\n",
        "plt.ylabel('Loss value')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPMf0jfF2zo6"
      },
      "source": [
        "### 5.2 Quantitative Analysis\n",
        "\n",
        "Go back to the `LinearModel` implementation and complete the definition of the `predict()` function. Recreate and retrain the model (as the class definition has been updated) -- i.e. run Section 4 again. Then, we can analyze the accuracy of the predictions as follows. You should see around 25\\%-26\\% accuracies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PAV1QQw218J"
      },
      "source": [
        "y_train_pred = model.predict(X_train)\n",
        "print('training accuracy: %f' % (np.mean(y_train == y_train_pred), ))\n",
        "\n",
        "y_val_pred = model.predict(X_val)\n",
        "print('validation accuracy: %f' % (np.mean(y_val == y_val_pred), ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eopyZ2-F22ce"
      },
      "source": [
        "### 5.3 Visual Results\n",
        "\n",
        "Let us look at some visual results. You should see that many of the predictions are off, as we should expect from low accuracy values. This is expected since a linear model is limited for such a classification problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nt0hbOSd24hY"
      },
      "source": [
        "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "S = 4 # SxS random samples will be selected and drawn\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [8, 6]\n",
        "\n",
        "for i in range(S):\n",
        "  for j in range(S):\n",
        "    id = np.random.randint(y_test.shape[0])\n",
        "    X = X_test[id]\n",
        "    y = y_test[id]\n",
        "    plt.subplot(S, S, i*S+j+1)\n",
        "    plt.imshow((X[:-1]+mean_image).reshape(32,32,3).astype('uint8'))\n",
        "    pred = model.predict(X.reshape(1, 3073))\n",
        "    plt.axis('off')\n",
        "    plt.title(\"GT: \" + classes[y] + \" Est: \" + classes[pred[0]])\n",
        "    plt.subplots_adjust(hspace = 0.3)\n",
        "\n",
        "plt.show()\n",
        "plt.rcParams['figure.figsize'] = [6, 4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NarKsB-04VhJ"
      },
      "source": [
        "### 5.4 Visualize Weights\n",
        "\n",
        "Let us visualize the weights for each class. You should see images that look like templates for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wpsEnO24YQa"
      },
      "source": [
        "w = model.W[:-1,:] # strip out the bias\n",
        "w = w.reshape(32, 32, 3, 10)\n",
        "w_min, w_max = np.min(w), np.max(w)\n",
        "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "for i in range(10):\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "      \n",
        "    # Rescale the weights to be between 0 and 255\n",
        "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
        "    plt.imshow(wimg.astype('uint8'))\n",
        "    plt.axis('off')\n",
        "    plt.title(classes[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4AJXtea2427"
      },
      "source": [
        "## 6 Tune your Model\n",
        "\n",
        "In Section 4, we trained our model with some arbitrary hyperparameters. In general, we are not going to be lucky enough to get suitable results with our first shot. Let us finetune our hyperparameters (learning rate and regularization strength) and see whether we can get better results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11sGe66pfrsD"
      },
      "source": [
        "### 6.1 Find the Best Hyperparameters\n",
        "\n",
        "Let us train the model for different values for our hyperparameters. Embrace yourselves for some long training time as we will train 27 models! Well, at least longer than what we had in Section 4. To reduce the waiting time, we will train the models for 3 epochs and train the best model longer later on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opP-WdBPduin"
      },
      "source": [
        "learning_rates = [1e-7, 1e-6, 1e-5]\n",
        "regularization_strengths = [1e3, 2e3, 3e3, 1e4, 2e4, 3e4, 1e5, 2e3, 3e5]\n",
        "\n",
        "# results is dictionary mapping tuples of the form\n",
        "# (learning_rate, regularization_strength) to tuples of the form\n",
        "# (training_accuracy, validation_accuracy). The accuracy is simply the fraction\n",
        "# of data points that are correctly classified.\n",
        "results = {}\n",
        "best_val = -1   # The highest validation accuracy that we have seen so far.\n",
        "best_svm = None # The LinearLayer object that achieved the highest validation rate.\n",
        "best_lr = None  # The learning rate for the best model\n",
        "best_rg = None  # The regularization strength for the best model\n",
        "\n",
        "for lr in learning_rates:\n",
        "  for rg in regularization_strengths:\n",
        "    ############################################################################\n",
        "    # @TODO: Write your code below in the parts marked with @TODO              #\n",
        "    ############################################################################\n",
        "    \n",
        "    ## @TODO: Create a new SVM instance\n",
        "    pass\n",
        "\n",
        "    ## @TODO: Create a new loss instance with current rg and margin=1\n",
        "    pass\n",
        "\n",
        "    ## @TODO: Train with the training set with current lr and rg for 3 epochs\n",
        "    pass\n",
        "    \n",
        "    # @TODO: Predict values for training set and the validation set\n",
        "    pass\n",
        "    \n",
        "    ############################################################################\n",
        "    #                              END OF YOUR CODE                            #\n",
        "    ############################################################################\n",
        "    \n",
        "    # Calculate training and validation accuracies\n",
        "    train_accuracy = np.mean(y_train_pred == y_train)\n",
        "    val_accuracy = np.mean(y_val_pred == y_val)\n",
        "\n",
        "    print(f\"learning rate={lr} and regularization={rg:E} provided train_accuracy={train_accuracy:.3f} and val_accuracy={val_accuracy:.3f}\")\n",
        "    \n",
        "    # Save the results\n",
        "    results[(lr,rg)] = (train_accuracy, val_accuracy)\n",
        "    if best_val < val_accuracy:\n",
        "        best_lr = lr\n",
        "        best_rg = rg\n",
        "        best_val = val_accuracy\n",
        "        best_model = model\n",
        "    \n",
        "print(f'\\nbest validation accuracy achieved during cross-validation: {best_val} with params reg={best_rg} and lr={best_lr}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IC1IfjO5e3-d"
      },
      "source": [
        "# Visualize the cross-validation results\n",
        "import math\n",
        "x_scatter = [math.log10(x[0]) for x in results]\n",
        "y_scatter = [math.log10(x[1]) for x in results]\n",
        "\n",
        "# plot training accuracy\n",
        "marker_size = 100\n",
        "colors = [results[x][0] for x in results]\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.scatter(x_scatter, y_scatter, marker_size, c=colors)\n",
        "plt.colorbar()\n",
        "plt.xlabel('log learning rate')\n",
        "plt.ylabel('log reg. strength')\n",
        "plt.title('CIFAR-10 training accuracy')\n",
        "plt.subplots_adjust(hspace = 1)\n",
        "\n",
        "# plot validation accuracy\n",
        "colors = [results[x][1] for x in results] # default size of markers is 20\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.scatter(x_scatter, y_scatter, marker_size, c=colors)\n",
        "plt.colorbar()\n",
        "plt.xlabel('log learning rate')\n",
        "plt.ylabel('log reg. strength')\n",
        "plt.title('CIFAR-10 validation accuracy')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJ09eQLRfwhD"
      },
      "source": [
        "### 6.2 Train the Best Model \n",
        "\n",
        "Let us train the model model with the best hyperparameters for longer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p20WrYhAe-8e"
      },
      "source": [
        "# Train the best model longer\n",
        "best_model = LinearLayer(num_of_dim, num_of_classes)\n",
        "loss_fn = SVM_Loss(margin=1, reg=best_rg)\n",
        "loss_history = train(best_model, loss_fn, X_train, y_train, learning_rate=best_lr, epochs=5, batch_size=200, verbose=True)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYeQ4Ihhf0JG"
      },
      "source": [
        "### 6.3 Analyze the Best Model\n",
        "\n",
        "Let us analyze some aspects of the best model. To keep things short, let us just look at the loss history and the accuracy. If you like, you can repeat some of the analyses in Section 5 here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BecAXYNOgqkx"
      },
      "source": [
        "plt.plot(loss_history)\n",
        "plt.xlabel('Iteration number')\n",
        "plt.ylabel('Loss value')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJbHKBRDwipa"
      },
      "source": [
        "Let us look at the accuracies again. We will see that tuning definitely improved the accuracies (from ~25\\% to 36\\%). We some more fine-grained tuning, it is possible to get up to 40\\% accuracies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Smwjci2hf2rj"
      },
      "source": [
        "y_train_pred = best_model.predict(X_train)\n",
        "print('training accuracy: %f' % (np.mean(y_train == y_train_pred), ))\n",
        "\n",
        "y_val_pred = best_model.predict(X_val)\n",
        "print('validation accuracy: %f' % (np.mean(y_val == y_val_pred), ))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}